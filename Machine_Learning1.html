<!DOCTYPE html>
<html>
    <head>
        <title>Machine Learning - Multiclass Classication</title>
    </head>

    <body>
        <h1>Comparison of different methods for multiclass classification problems</h1>
        <img src="images/ML1/Picture.JPG" width="500" height="300" alt="A beautiful picture">

        <p>The project is divided into three sections. Section 1 discusses the One-vs-All and One-vs-One method which
            are techniques to transform a multiclass classification problem into multiple binary classification problems.
            Section 2 outlines the multinomial logistic regression and discusses why it is not sufficient in many cases.
            Section 3 applies different multiclass classification methods to the Letter Image Recognition dataset,
            and compares the model performance as well as provides some insights on each method.
            Programming in this project are done using Pythonâ€™s scikit-learn library.
        </p>
        <hr/>

        <h2>Transform a multiclass classification problem into multiple binary classification problems </h2>
        <p><b>One-vs-All method</b>
            <ul style="list-style-type: disc">
                <li>Training a single binary classifier for each class by treating training samples in that class as
                    the positive samples and training samples not in that class as negative samples
                </li>
                <li>Usually has problems when the training data has an unbalanced or skewed class label distribution.
                    Can be solved by applying common techniques of balancing the training data such as over-sample
                    the minority class or under-sample the majority class.</li>
            </ul>
        </p>

        <p><b>One-vs-One method</b>
            <ul style="list-style-type: disc">
                <li>Also called the All-Pairs or All-vs-All method
                </li>
                <li>Usually much less sensitive to the problem of unbalanced class distribution as each binary
                    classifier is built only on a pair of classes.</li>
            </ul>
        </p>
        <hr/>


        <h2>Extend binary classification techniques to multiclass classification problems </h2>
        <p><b>Statistical Method - Multinomial logistic regression</b>
            <ul style="list-style-type: disc">
                <li>Use one of the K classes as the base or reference class and set up K-1 independent binary logistic
                    regression models by comparing each of the K-1 classes against the reference class.
                </li>
                <li>Allowing both prediction and inference to be made easily, which is important for almost all
                    statistical methods.</li>
                <li>Linear classifier (linear decision boundary for the classification). Inputs into the model are
                    assumed to be linearly separable and the predicted value is a linear function of the inputs
                    (here, the predicted log-odds is a linear model of the inputs)</li>
            </ul>
        </p>

        <p><b>Machine Learning Algorithms - Non-linear classifiers</b>
            <ul style="list-style-type: disc">
                <li>k-nearest neighbors (KNN)</li>
                <li>naive bayes</li>
                <li>decision trees (random forest)</li>
                <li>neural networks (multilayer perceptron (MLP))</li>
                <li>support vector machines (SVM)</li>
            </ul>
        </p>
        <hr/>


        <h2>Letter Image Recognition dataset</h2>
        <p>The data contains 20000 rectangular pixel images where each image (observation) is classified as one of
            the 26 capital letters (therefore this is a 26-class classification problem) in the English alphabet.
            In this data, each observation has 16 attributes or features, where each feature is either a statistical
            moment or edge counts that has already been scaled into a range of integer values from 0 to 15.
        </p>

        <img src="images/ML1/Picture2.JPG" width="1000" height="500" alt="A beautiful picture">
        <hr/>
        <h2>Exploratory Data Analysis (EDA)</h2>
            <ul style="list-style-type: disc">
                <li>Class label distribution</li>
                <li>Feature distributions</li>
                <li>Correlation of features</li>
                <li>Data Characteristics</li>
            </ul>

        <img src="images/ML1/P1.PNG" width="500" height="500" alt="A beautiful picture">
        <img src="images/ML1/P2.PNG" width="500" height="500" alt="A beautiful picture">
        <img src="images/ML1/P3.PNG" width="500" height="500" alt="A beautiful picture">
        <hr/>
        <h2>Summary of model performance for all multiclass classification techniques</h2>
            <ul style="list-style-type: disc">
                <li>The following table summarizes the predictive power of each model used in the data analysis
                    (from most predictive to least predictive). </li>
                <li>In conclusion, for this Letter Image Recognition dataset, the best model (in terms of predictive
                    power) is the One-vs-One method with SVM classifier. </li>
            </ul>

        <img src="images/ML1/Picture3.JPG" width="400" height="300" alt="A beautiful picture">
        <hr/>

        <h2><b>Closing Thoughts</b></h2>

        <img src="images/ML1/Picture4.JPG" width="500" height="300" alt="A beautiful picture">

        <p>Nowadays it is almost impossible to hear (and know/use) machine learning algorithms in many jobs, and it is
            becoming much easier to build ML models due to various software platforms that are available.
        </p>

        <p>In this project I did not try any of the "fancy algorithms" but rather focus on a few commonly known algorithms
            but try to study their mathematical properties and draw some insights on when to use each method in the
            general setting. In addition, the dataset I have chosen is a very good dataset as it is clean and organized,
            which I think is very good for academic projects. In reality, more work will need to be done on the feature
            engineering stage, which can make the modelling more challenging sometimes.
        </p>

        <hr/>
        <p>Last updated on Jan 1, 2019</p>

    </body>
</html>

